import google.generativeai as genai
import json
import numpy as np
import os
import random
import re
import sys
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict, field
from datetime import datetime
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from typing import List, Tuple, Dict, Any, Optional


def robust_json_parser(llm_response: str) -> Optional[Dict[str, Any]]:
    """
    Attempts to extract and parse a JSON object from a string that may contain
    markdown fences or other extraneous text.

    Args:
        llm_response: The string response from the LLM, expected to contain JSON.

    Returns:
        A dictionary if parsing is successful, otherwise None.
    """
    # 1. Look for JSON within markdown fences ` ``json ... ``` `
    llm_response = re.sub(r'\s+', ' ', llm_response)
    match = re.search(r"```(?:json)?\s*(\{.*?\}|\[.*?\])\s```", llm_response, re.DOTALL)
    if match:
        json_str = match.group(1)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            logging.warning(f"JSON in markdown failed to parse: {e}. Content: '{json_str[:100]}...'")
            # Fall through to other methods in case of malformed JSON in markdown

    try:
        return json.loads(llm_response)
    except json.JSONDecodeError as e:
        logging.warning(f"JSON in markdown failed to parse: {e}. Content: '{llm_response[:1000]}...'")
        # Fall through to other methods in case of malformed JSON in markdown


    # 2. If no markdown, try to find the first '{' and last '}'
    try:
        start_index = llm_response.find('{')
        end_index = llm_response.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            potential_json = llm_response[start_index:end_index+1]
            return json.loads(potential_json)
    except json.JSONDecodeError:
        # Fall through if this substring is not valid JSON
        pass

    # 3. As a last resort, try parsing the whole string
    try:
        return json.loads(llm_response)
    except json.JSONDecodeError:
        pass

    logging.error(f"Could not parse JSON from LLM response after multiple attempts. Response: '{llm_response}'")
    return None


@dataclass
class Step:
    name: str                 # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è (e.g., "1.1 –ê–Ω–∞–ª–∏–∑")
    handler_name: str         # –ò–º—è –º–µ—Ç–æ–¥–∞ –≤ NovelGenerator (e.g., "step_foundation_1_1_analysis")
    status: str = 'planned'   # 'planned', 'started', 'done'

def load_prompt(name: str, **kwargs) -> str:
    template = open(f'prompts/{name}.md', 'r').read()
    def evaluator(match):
        expression = match.group(1)
        try:
            return str(eval(expression, kwargs))
        except Exception as e:
            print(f"[!] –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç–µ '{name}': {expression} -> {e}")
            return f"{{EVAL_ERROR: {expression}}}"
    return re.sub(r"\{(.*?)\}", evaluator, template)

def log_chapter(kind, number, text):
    if not os.path.exists('generated_chapters'):
        os.makedirs('generated_chapters')
    with open(f"generated_chapters/chapter_{kind}_{number:02}.md", "w", encoding="utf-8") as f:
        f.write(text)

API_KEY = os.getenv('AI_API_KEY', '–í–ê–®_API_–ö–õ–Æ–ß')

ALLOW_MATURE_LANGUAGE = True
MODEL_NAME = "gemini-2.5-pro" # "gemini-3-pro-preview"

DEFAULT_CONCEPTS = load_prompt('global_concepts')
CONCEPTS = open('concepts.txt', 'r').read() if os.path.isfile('concepts.txt') else DEFAULT_CONCEPTS

DEFAULT_OVERRIDE = load_prompt('global_override')
SYSTEM_OVERRIDE = open('override.txt', 'r').read() if os.path.isfile('override.txt') else DEFAULT_OVERRIDE

ANTI_PLEASING = load_prompt('global_anti_pleasing')

PLANNER_ROLE = "–¢–≤–æ—è —Ä–æ–ª—å: –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä-–ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–π —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∫–Ω–∏–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."

SYNOPSIS = open('synopsis.txt', 'r').read()
START_INFO = open('start-info.txt', 'r').read() if os.path.isfile('start-info.txt') else ''

NUM_CHAPTERS = int(sys.argv[1]) if len(sys.argv) > 1 else 4

MAX_WORKERS = 5
MAX_CLUSTERS = 8

# --- –ö–û–ù–ï–¶ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ---

SAFETY_SETTINGS = [
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE"
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE"
        }
    ]

JSON_REPEATER = load_prompt('schema_repeater')
REPEATER_PROMPT = load_prompt('global_repeater', ANTI_PLEASING=ANTI_PLEASING, json_repeater=JSON_REPEATER)

PLAN_SCHEMA = json.loads(load_prompt('schema_plan'))
SCHEMA_ANALYSIS = json.loads(load_prompt('schema_analysis'))

JSON_OUT = """
{
    "chapters": [
        {
            "number": "1",
            "title": "–ì–ª–∞–≤–∞ 1",
            "scenes": [
                "2025-10-17 11:30. –í–∞—Å—è, –∏–∑–º–æ—Ç–∞–Ω–Ω—ã–π –ø–æ—Å–ª–µ –±–µ—Å—Å–æ–Ω–Ω–æ–π –Ω–æ—á–∏, —Å–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–º—ã—Ç—É—é –ø–∏–≤–æ–≤–∞—Ä–Ω—é. –¶–µ–ª—å ‚Äî —Å–≤–∞—Ä–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π –ª–∞–≥–µ—Ä, –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –æ—Å–Ω–æ–≤–∞–º. –û–Ω –Ω–µ –∏—â–µ—Ç –ø—Ä–æ—Ä—ã–≤–∞, –∞ –ø—ã—Ç–∞–µ—Ç—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å —á–µ—Ä–µ–∑ —Ä—É—Ç–∏–Ω—É. –û–Ω –º–µ—Ç–æ–¥–∏—á–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —á–∏—Å—Ç–æ—Ç—É —á–∞–Ω–∞, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—à–∫–∏ —Å —Å–æ–ª–æ–¥–æ–º. –í—Ö–æ–¥–∏—Ç –ú–∞—Ç—Ä—ë–Ω–∞ —Å –ø–∏—Å—å–º–æ–º –æ—Ç –ü–∏–ª—Å–Ω–µ—Ä–∞. –ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢: –í–∞—Å—è —á–∏—Ç–∞–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –∂–∞–ª–æ–±—É –Ω–∞ '–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —É—Ä–æ–≤–µ–Ω—å –∫–∞—Ä–±–æ–Ω–∏–∑–∞—Ü–∏–∏'. –í–º–µ—Å—Ç–æ –≥–Ω–µ–≤–∞ –∏–ª–∏ —Å—Ç—Ä–∞—Ö–∞ –æ–Ω —É—Å–º–µ—Ö–∞–µ—Ç—Å—è. –ü–û–ö–ê–ó–ê–¢–¨: –≠—Ç–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π, '—Ö–æ–ª–æ–¥–Ω–æ–π' –≤–æ–π–Ω—ã, –∫–æ—Ç–æ—Ä–∞—è –µ–≥–æ –Ω–µ –ø—É–≥–∞–µ—Ç, –∞ –∑–∞–±–∞–≤–ª—è–µ—Ç. –û–Ω –±–µ—Ä–µ—Ç –ø–µ—Ä–æ –∏ –ø–∏—à–µ—Ç –∏—Ä–æ–Ω–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç. –°—Ü–µ–Ω–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ–º, –∫–∞–∫ –ú–∞—Ç—Ä—ë–Ω–∞ —É–Ω–æ—Å–∏—Ç –µ–≥–æ –æ—Ç–≤–µ—Ç, –∫–∞—á–∞—è –≥–æ–ª–æ–≤–æ–π, –Ω–æ —Å —Ç–µ–Ω—å—é —É–ª—ã–±–∫–∏. –í—Ä–µ–º—è: ~20 –º–∏–Ω—É—Ç.",
                ...
            ]
        },
        ....
    ]
}
"""

JSON_STATE_STRUCT = '''
{
    "chapter_summary": "–ü–µ—Ä–µ—á–∏—Å–ª–∏ –í–°–ï –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è, –ø—Ä–æ–∏–∑–æ—à–µ–¥—à–∏–µ –≤ –≠–¢–û–ô –≥–ª–∞–≤–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º–µ—Å—Ç–∞, –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏: –∫—Ç–æ, —á—Ç–æ, –≥–¥–µ, –∫–æ–≥–¥–∞, –∫–∞–∫...",
    "world_state_updates": {
        "chapter_end_date_time": "YYYY-MM-DD HH:MM",
        "characters": {
            "character_name_1": {
                "location": "–ù–æ–≤–∞—è –ª–æ–∫–∞—Ü–∏—è",
                "physical_condition": "–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏ –±–µ–∑–∂–∞–ª–æ—Å—Ç–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –£–∫–∞–∂–∏ –Ω–µ –ø—Ä–æ—Å—Ç–æ '—Ä–∞–Ω–µ–Ω', –∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä —Ä–∞–Ω—ã, —Å—Ç–µ–ø–µ–Ω—å –±–æ–ª–∏ (1-10), –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –¥–≤–∏–∂–µ–Ω–∏–∏. –ü—Ä–∏–º–µ—Ä: '–°–∫–≤–æ–∑–Ω–æ–µ —Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä–µ–ª–æ–π –≤ –ª–µ–≤–æ–º –ø–ª–µ—á–µ, –∫—Ä–∞—è –ø—Ä–∏–∂–∂–µ–Ω—ã. –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –Ω–æ—é—â–∞—è –±–æ–ª—å (7/10), —É—Å–∏–ª–∏–≤–∞—é—â–∞—è—Å—è –ø—Ä–∏ –¥–≤–∏–∂–µ–Ω–∏–∏. –†—É–∫–∞ –ø–æ—á—Ç–∏ –Ω–µ –¥–µ–π—Å—Ç–≤—É–µ—Ç, –º–æ–∂–µ—Ç –ª–∏—à—å –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å –ø—Ä–µ–¥–º–µ—Ç—ã. –ù–∞—á–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–∏—Ö–æ—Ä–∞–¥–∫–∏.'",
                "health_issues": [
                    {"what": "–≤—ã–±–∏–ª –ø–∞–ª–µ—Ü ", "when": "–≥–ª–∞–≤–∞ 3, –¥–∞—Ç–∞ YYYY-MM-DD", "estimated_recovery": "–¥–∞—Ç–∞ –∑–∞–∂–∏–≤–ª–µ–Ω–∏—è, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"},
                    ....
                ],
                "psychological_condition": ["–Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"],
                "inventory_changes": {"add": ["–Ω–æ–≤—ã–π –ø—Ä–µ–¥–º–µ—Ç"], "remove": ["—Å—Ç–∞—Ä—ã–π –ø—Ä–µ–¥–º–µ—Ç"], "keep":["–∏–º–µ—é—â–∏–π—Å—è –ø—Ä–µ–¥–º–µ—Ç"]},
                "knowledge_update": "–ß—Ç–æ –Ω–æ–≤–æ–≥–æ –∏ –≤–∞–∂–Ω–æ–≥–æ —É–∑–Ω–∞–ª –ø–µ—Ä—Å–æ–Ω–∞–∂",
                "familiar_people": [
                    {"person": "–ò–∏—è –∑–Ω–∞–∫–æ–º–æ–≥–æ", "since": "–î–∞—Ç–∞ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞", "relationships": "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π", "last_contact": "–¥–∞—Ç–∞"},
                    {"person": "–ò–∏—è –Ω–æ–≤–æ–≥–æ –∑–Ω–∞–∫–æ–º–æ–≥–æ", "since": "–î–∞—Ç–∞ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞", "relationships": "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π", "last_contact": "–¥–∞—Ç–∞"},
                    // –î–ª—è –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö –∑–Ω–∞–∫–æ–º—Å—Ç–≤
                ]
            },
            "character_name_2": {
                // ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è ...
            }
        },
        "items": {
            "item_name_1": {
                "location": "–ù–æ–≤–æ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –≤–ª–∞–¥–µ–ª–µ—Ü",
                "status": "–ù–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω')"
            }
        },
        "locations": {
            "location_name_1": {
                "status": "–ù–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, '—á–∞—Å—Ç–∏—á–Ω–æ —Ä–∞–∑—Ä—É—à–µ–Ω–∞')"
            }
        }
    }
}
'''

TECHNIQUES_POOL = [
    "–ù–∞—á–Ω–∏ —Å—Ü–µ–Ω—É —Å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π –¥–µ—Ç–∞–ª–∏ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏—è",
    "–í–∫–ª—é—á–∏ –¥–∏–∞–ª–æ–≥ —Å –ø–æ–¥—Ç–µ–∫—Å—Ç–æ–º - –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ –≥–æ–≤–æ—Ä—è—Ç –Ω–µ —Ç–æ, —á—Ç–æ –¥—É–º–∞—é—Ç",
    "–ü–æ–∫–∞–∂–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É –º—ã—Å–ª—è–º–∏ –∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –≥–µ—Ä–æ—è",
    "–î–æ–±–∞–≤—å —Å–µ–Ω—Å–æ—Ä–Ω—É—é –¥–µ—Ç–∞–ª—å, –∫–æ—Ç–æ—Ä–∞—è —É—Å–∏–ª–∏—Ç –∞—Ç–º–æ—Å—Ñ–µ—Ä—É",
    "–î–∞–π –ø–µ—Ä—Å–æ–Ω–∞–∂—É —Å–æ–≤–µ—Ä—à–∏—Ç—å –∏—Ä—Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç—É–ø–æ–∫",
    "–í–∫–ª—é—á–∏ –º–æ–º–µ–Ω—Ç –Ω–µ–ª–æ–≤–∫–æ–π –ø–∞—É–∑—ã –∏–ª–∏ –Ω–µ–¥–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è",
    "–ü–æ–∫–∞–∂–∏, –∫–∞–∫ —Å—Ä–µ–¥–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞",
    "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç (—Ç–µ–ø–ª–æ/—Ö–æ–ª–æ–¥, —Å–≤–µ—Ç/—Ç–µ–Ω—å, —Ç–∏—à–∏–Ω–∞/—à—É–º)",
    "–ü–æ–∫–∞–∂–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ —á–µ—Ä–µ–∑ –µ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –º–µ–ª–æ—á–∞–º",
    "–í–∫–ª—é—á–∏ –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –ø–ª–∞–Ω –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ —Å—Ä—ã–≤–∞–µ—Ç—Å—è",
    "–ü–æ–∫–∞–∂–∏ —Ä–µ–∞–∫—Ü–∏—é –æ–∫—Ä—É–∂–∞—é—â–∏—Ö (–≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ, –∑–∞–≤–∏—Å—Ç—å, —Å—Ç—Ä–∞—Ö) –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—è –ì–ì",
]

CHAOS_ELEMENTS = [
    "–ü–æ–º–Ω–∏: –ª—é–¥–∏ —á–∞—Å—Ç–æ –¥–µ–ª–∞—é—Ç –≥–ª—É–ø–æ—Å—Ç–∏ –±–µ–∑ –ø—Ä–∏—á–∏–Ω—ã",
    "–ù–µ –≤—Å–µ –¥–µ—Ç–∞–ª–∏ –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ —Å—é–∂–µ—Ç - –∏–Ω–æ–≥–¥–∞ –∂–∏–∑–Ω—å –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç",
    "–ü–æ–∑–≤–æ–ª—å –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –æ—Ç–≤–ª–µ–∫–∞—Ç—å—Å—è –Ω–∞ –º–µ–ª–æ—á–∏",
    "–ü—É—Å—Ç—å –∫—Ç–æ-—Ç–æ —Å–∫–∞–∂–µ—Ç —á—Ç–æ-—Ç–æ –Ω–µ—É–º–µ—Å—Ç–Ω–æ–µ –∏–ª–∏ –∑–∞–±—É–¥–µ—Ç –≤–∞–∂–Ω–æ–µ",
    "–î–æ–±–∞–≤—å –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –ø–ª–∞–Ω –∏–¥–µ—Ç –Ω–µ —Ç–∞–∫, –∫–∞–∫ –∑–∞–¥—É–º–∞–Ω–æ",
    "–ü–µ—Ä—Å–æ–Ω–∞–∂ –¥–µ–ª–∞–µ—Ç —á—Ç–æ-—Ç–æ –Ω–µ–ª–æ–≥–∏—á–Ω–æ–µ –∏–∑-–∑–∞ —ç–º–æ—Ü–∏–π",
    "–ö—Ç–æ-—Ç–æ –∑–∞–±—ã–≤–∞–µ—Ç –≤–∞–∂–Ω—É—é –¥–µ—Ç–∞–ª—å",
    "–ü–ª–∞–Ω —Å—Ä—ã–≤–∞–µ—Ç—Å—è –∏–∑-–∑–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏ –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º (–∫—Ç–æ-—Ç–æ —Å–ø–æ—Ç–∫–Ω—É–ª—Å—è, —É—Ä–æ–Ω–∏–ª –∫–ª—é—á–µ–≤–æ–π –ø—Ä–µ–¥–º–µ—Ç, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª –∫–æ–º–∞–Ω–¥—É)",
    "–ü–µ—Ä—Å–æ–Ω–∞–∂ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–µ–≤–µ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –Ω–µ–ø–æ–ª–Ω–æ–π –∏–ª–∏ –ª–æ–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
    "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è (–≥–Ω–µ–≤, —Å—Ç—Ä–∞—Ö) –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –æ—Ç–∫–ª–æ–Ω–∏—Ç—å—Å—è –æ—Ç –ø–ª–∞–Ω–∞ —Å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è–º–∏",
    "–í—Ç–æ—Ä–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å–æ–±—ã—Ç–∏—è",
    "–í–∞–∂–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç –ø–æ—Ç–µ—Ä—è–Ω –≤ —Å–∞–º—ã–π –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –º–æ–º–µ–Ω—Ç",
    "–ü–µ—Ä—Å–æ–Ω–∞–∂ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
    "–°–æ—é–∑–Ω–∏–∫ —Å–ª—É—á–∞–π–Ω–æ –º–µ—à–∞–µ—Ç –ø–ª–∞–Ω—É –≥–µ—Ä–æ—è",
    "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è/–æ—Ä—É–∂–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ —Ç–∞–∫, –∫–∞–∫ –æ–∂–∏–¥–∞–ª–æ—Å—å",
]

STATS = {} # 'models/gemini-2.5-pro': {'input_tokens': 0, 'output_tokens': 0, 'calls': 0},

@dataclass
class TextChunk:
    chunk_id: int
    text: str
    chapter: int
    paragraph_id: int

@dataclass
class SimilarityCluster:
    cluster_id: int
    original: TextChunk
    similar_chunks: List[Tuple[TextChunk, float]]

@dataclass
class AnalysisResult:
    cluster: SimilarityCluster
    status: str
    motive: str
    diagnosis: str
    recommendation: str
    confidence: float

def generate_embedding(text: str):
    result = genai.embed_content(
        model='models/gemini-embedding-001',
        content=text,
        task_type="SEMANTIC_SIMILARITY",
        output_dimensionality=768
    )
    return result['embedding']

import random
import re

class RhythmEngine:
    def __init__(self):
        self.patterns = {
            "action": {
                "weights": ["S", "M", "L"],
                "probs": [0.7, 0.25, 0.05], # –ú–Ω–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö, –º–∞–ª–æ –¥–ª–∏–Ω–Ω—ã—Ö
                "description": "ACTION STACCATO: –ë—ã—Å—Ç—Ä—ã–π —Ç–µ–º–ø, —Ä—É–±–ª–µ–Ω—ã–µ —Ñ—Ä–∞–∑—ã, –º–∏–Ω–∏–º—É–º –ø—Ä–∏—á–∞—Å—Ç–Ω—ã—Ö –æ–±–æ—Ä–æ—Ç–æ–≤."
            },
            "dialogue": {
                "weights": ["S", "M", "L"],
                "probs": [0.4, 0.5, 0.1],
                "description": "DIALOGUE FLOW: –û–±–º–µ–Ω —Ä–µ–ø–ª–∏–∫–∞–º–∏, –ø–∞—É–∑—ã, —Ä–µ–∞–∫—Ü–∏–∏."
            },
            "description": {
                "weights": ["S", "M", "L"],
                "probs": [0.1, 0.4, 0.5], # –î–ª–∏–Ω–Ω—ã–µ, —Ç–µ–∫—É—á–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                "description": "ATMOSPHERIC FLOW: –ü–ª–∞–≤–Ω—ã–π, –æ–±–≤–æ–ª–∞–∫–∏–≤–∞—é—â–∏–π —Ä–∏—Ç–º, —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."
            },
            "tension": {
                "weights": ["S", "M", "L"],
                "probs": [0.5, 0.1, 0.4], # –ö–æ–Ω—Ç—Ä–∞—Å—Ç: –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ
                "description": "TENSION SPIKES: –†–µ–∑–∫–∏–µ –ø–µ—Ä–µ–ø–∞–¥—ã –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –Ω–∞–≥–Ω–µ—Ç–∞–Ω–∏–π –∫ –∫–æ—Ä–æ—Ç–∫–∏–º —É–¥–∞—Ä–∞–º."
            },
            "balanced": {
                "weights": ["S", "M", "L"],
                "probs": [0.3, 0.5, 0.2],
                "description": "NARRATIVE BALANCE: –°–ø–æ–∫–æ–π–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ."
            }
        }

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—Ç–º–∞ (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
        self.triggers = {
            "action": ["–±–æ–π", "—É–¥–∞—Ä", "–±–µ–≥", "–≤—ã—Å—Ç—Ä–µ–ª", "–∫—Ä–æ–≤—å", "—Å—Ö–≤–∞—Ç–∫–∞", "–≤–∑—Ä—ã–≤", "–∫—Ä–∏–∫", "–ø–æ–≥–æ–Ω—è", "–¥—Ä–∞–∫–∞"],
            "dialogue": ["—Ä–∞–∑–≥–æ–≤–æ—Ä", "–±–µ—Å–µ–¥–∞", "–æ–±—Å—É–∂–¥", "—Å–ø–æ—Ä", "–¥–æ–ø—Ä–æ—Å", "–ø—Ä–∏–∑–Ω–∞–Ω–∏–µ", "—à–µ–ø—Ç", "–≥–æ–ª–æ—Å"],
            "description": ["–æ—Å–º–æ—Ç—Ä", "–≤–∏–¥", "–ø–µ–π–∑–∞–∂", "–∫–æ–º–Ω–∞—Ç–∞", "—Ç–∏—à–∏–Ω–∞", "–∞—Ç–º–æ—Å—Ñ–µ—Ä–∞", "–≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ", "–º—ã—Å–ª–∏"],
            "tension": ["—Å—Ç—Ä–∞—Ö", "—Ç–µ–º–Ω–æ—Ç–∞", "—à–∞–≥–∏", "–æ–∂–∏–¥–∞–Ω–∏–µ", "—É–∂–∞—Å", "–∫—Ä–∞—Å—Ç—å—Å—è", "—Å–ª–µ–∂"]
        }

    def detect_mode(self, scene_description: str) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∂–∏–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
        scene_description = scene_description.lower()
        scores = {k: 0 for k in self.triggers.keys()}

        for mode, keywords in self.triggers.items():
            for word in keywords:
                if word in scene_description:
                    scores[mode] += 1

        if max(scores.values()) == 0:
            return "balanced"
        return max(scores, key=scores.get)

    def _get_pattern_sequence(self, mode, length=15):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—ã—Ä—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å [S]->[M]..."""
        config = self.patterns.get(mode, self.patterns["balanced"])
        sequence = random.choices(
            config["weights"],
            weights=config["probs"],
            k=length
        )
        return " -> ".join([f"[{x}]" for x in sequence])

    def generate_rhythm_block(self, scene_text="", mode=None, length=20):
        """–î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º (–æ–¥–∏–Ω –±–ª–æ–∫ –Ω–∞ –≤—Å–µ)."""
        if not mode:
            mode = self.detect_mode(scene_text)

        config = self.patterns.get(mode, self.patterns["balanced"])
        rhythm_map = self._get_pattern_sequence(mode, length)

        return f"""
### üéπ –°–ò–ù–¢–ê–ö–°–ò–ß–ï–°–ö–ò–ô –ö–û–ù–¢–†–û–õ–õ–ï–† (–†–ï–ñ–ò–ú: {mode.upper()})
–¢–≤–æ—è –∫–∞—Ä—Ç–∞ —Ä–∏—Ç–º–∞: {rhythm_map}
–í–∞–∂–Ω–æ: {config['description']}
"""

    def generate_chapter_map(self, scenes_data: list) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –≤—Å–µ–π –≥–ª–∞–≤—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ flash-lite.
        scenes_data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π [{"num": 1, "mode": "action"}, ...]
        """
        output = ["### üéπ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ê–Ø –ü–ê–†–¢–ò–¢–£–†–ê –ì–õ–ê–í–´"]
        output.append("–¢—ã –æ–±—è–∑–∞–Ω –º–µ–Ω—è—Ç—å —Ä–∏—Ç–º –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –æ—Ç —Å—Ü–µ–Ω—ã –∫ —Å—Ü–µ–Ω–µ —Å–æ–≥–ª–∞—Å–Ω–æ —ç—Ç–æ–π –∫–∞—Ä—Ç–µ:")

        output.append("\n–õ–ï–ì–ï–ù–î–ê –î–õ–ò–ù–´ –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô:")
        output.append("[S] = –ö–û–†–û–¢–ö–û–ï (3-6 —Å–ª–æ–≤). –£–¥–∞—Ä. –§–∞–∫—Ç.")
        output.append("[M] = –°–†–ï–î–ù–ï–ï (7-15 —Å–ª–æ–≤). –û–±—ã—á–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ.")
        output.append("[L] = –î–õ–ò–ù–ù–û–ï (16+ —Å–ª–æ–≤). –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞, –º—ã—Å–ª–∏.")

        for scene in scenes_data:
            mode = scene.get('mode', 'balanced').lower()
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ —á—Ç–æ-—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ–µ, –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –Ω–∞ balanced
            if mode not in self.patterns:
                mode = 'balanced'

            config = self.patterns[mode]
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —ç—Ç–æ–π —Å—Ü–µ–Ω—ã
            pattern = self._get_pattern_sequence(mode, length=12)

            scene_block = f"""
**–°–¶–ï–ù–ê {scene.get('num', '?')}: {scene.get('title', '–°—Ü–µ–Ω–∞')}**
- –†–µ–∂–∏–º: {mode.upper()}
- –ó–∞–¥–∞—á–∞ —Ä–∏—Ç–º–∞: {config['description']}
- –¢–≤–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern} ... (–ø–æ–≤—Ç–æ—Ä—è—Ç—å –¥–∏–Ω–∞–º–∏–∫—É)
"""
            output.append(scene_block)

        output.append("\n–í–ê–ñ–ù–û: –°–ª–µ–¥–∏ –∑–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º —Ä–∏—Ç–º–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞–º–∏!")
        return "\n".join(output)


class StyleRepeatingChecker:
    def __init__(self):
        self.qdrant_client = QdrantClient(host="localhost", port=6333)
        self.novel_collection_name: str | None = None # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑–≤–Ω–µ
        self.vector_size = 768
        self.chunk_id_counter = 0
        self._collection_checked = False # –§–ª–∞–≥, —á—Ç–æ –º—ã –ø—Ä–æ–≤–µ—Ä–∏–ª–∏/—Å–æ–∑–¥–∞–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—é

    def set_collection_name(self, name: str):
        self.novel_collection_name = name
        print(f"Qdrant: –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {self.novel_collection_name}")

    def _ensure_collection_exists(self):
        if self._collection_checked:
            return
        if not self.novel_collection_name:
            raise ValueError("–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í—ã–∑–æ–≤–∏—Ç–µ set_collection_name().")

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
            self.qdrant_client.get_collection(collection_name=self.novel_collection_name)
            print(f"‚úì Qdrant: –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {self.novel_collection_name}")
        except Exception:
            # –ù–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º
            print(f"Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.novel_collection_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π...")
            self._create_novel_collection()

        self._collection_checked = True

    def _create_novel_collection(self):
        try:
            self.qdrant_client.recreate_collection(
                collection_name=self.novel_collection_name,
                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)
            )
            print(f"‚úì Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.novel_collection_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞.")
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant: {e}")
            raise

    def _sent_tokenize_with_punctuation(self, text: str) -> List[str]:
        if not text:
            return []
        sentences = re.split(r'(?<=[.!?‚Ä¶])\s+', text.strip())
        return [s.strip() for s in sentences if s.strip()]

    def _chunk_text(self, text: str, chapter_num: int) -> List[TextChunk]:
        chunks = []
        paragraphs = re.split(r'\n+', text)
        para_id = 0
        joiner = ""
        for paragraph in paragraphs:
            paragraph = joiner + "\n" + paragraph if joiner else paragraph
            joiner = ""
            if not paragraph.strip():
                continue

            if len(paragraph.strip()) < 20 or len(paragraph.strip().split(' ')) < 3:
                joiner = paragraph.strip()
                continue

            chunks.append(TextChunk(
                chunk_id=self.chunk_id_counter,
                text=paragraph.strip(),
                chapter=chapter_num,
                paragraph_id=para_id
            ))
            self.chunk_id_counter += 1

            # --- 2. –ß–∞–Ω–∫–∏ –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ ---
            sentences = self._sent_tokenize_with_punctuation(paragraph.strip())

            window_size = 3
            if len(sentences) >= window_size:
                for i in range(len(sentences) - window_size + 1):
                    window = sentences[i : i + window_size]
                    chunk_text = " ".join(window)
                    chunks.append(TextChunk(
                        chunk_id=self.chunk_id_counter,
                        text=chunk_text,
                        chapter=chapter_num,
                        paragraph_id=para_id
                    ))
                    self.chunk_id_counter += 1
            para_id += 1
        print(f"\n---–°–û–ó–î–ê–ù–û –ß–ê–ù–ö–û–í {len(chunks)}")

        return chunks

    def _embed_chunks(self, chunks: List[TextChunk]) -> Dict[int, List[float]]:
        embeddings = {}
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_chunk = {executor.submit(generate_embedding, chunk.text): chunk for chunk in chunks}
            for future in as_completed(future_to_chunk):
                try:
                    chunk = future_to_chunk[future]
                    embeddings[chunk.chunk_id] = future.result()
                    print('*', end='')
                except Exception as e:
                    print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–Ω–µ –ø–æ—Ç–æ–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞ {chunk.chunk_id}: {e}")
                    pass
        print(f'\n–°–æ–∑–¥–∞–Ω–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings)}')
        return embeddings

    def _add_chapter_to_db(self, chunks: List[TextChunk], chapter_number: int) -> Dict[int, List[float]]:
        """--- –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ---"""
        self._ensure_collection_exists() # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è –µ—Å—Ç—å

        embeddings = self._embed_chunks(chunks)
        points = [
            PointStruct(
                id=chunk.chunk_id,
                vector=embeddings[chunk.chunk_id],
                payload=asdict(chunk)
            ) for chunk in chunks if chunk.chunk_id in embeddings
        ]
        if points:
            self.qdrant_client.upsert(
                collection_name=self.novel_collection_name,
                points=points,
                wait=True
            )
        return embeddings

    def _analyze_one_repetition_cluster(self, cluster: SimilarityCluster) -> AnalysisResult | None:
        try:
            fragments_text = f"–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –§–†–ê–ì–ú–ï–ù–¢ (–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —á–µ—Ä–Ω–æ–≤–∏–∫–∞, –ì–ª–∞–≤–∞ {cluster.original.chapter}):\n"
            fragments_text += f'"{cluster.original.text}"\n\n'
            fragments_text += "–ü–û–•–û–ñ–ò–ï –§–†–ê–ì–ú–ï–ù–¢–´ –ò–ó –ü–†–ï–î–´–î–£–©–ò–• –ì–õ–ê–í:\n"

            for similar_chunk, score in cluster.similar_chunks:
                fragments_text += f"- –ì–ª–∞–≤–∞ {similar_chunk.chapter} (—Å—Ö–æ–∂–µ—Å—Ç—å: {score:.2f}):\n"
                fragments_text += f'  "{similar_chunk.text}"\n\n'

            prompt = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–º–æ–ø–æ–≤—Ç–æ—Ä–∞:\n\n{fragments_text}\n\n–î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—É—â–µ–π –ì–ª–∞–≤—ã {cluster.original.chapter}"

            analysis_model = genai.GenerativeModel(
                model_name="gemini-flash-lite-latest",
                system_instruction=REPEATER_PROMPT,
                safety_settings=SAFETY_SETTINGS,
            )

            RESPONSE_SCHEMA = {
                "type": "object",
                "properties": {
                    "status": {"type": "string", "enum": ["OK", "SLIGHT_REPETITION", "STRONG_CLICHE"]},
                    "motive": {"type": "string"},
                    "diagnosis": {"type": "string"},
                    "recommendation": {"type": "string"},
                    "confidence": {"type": "number"},
                },
                "required": ["status", "motive", "diagnosis", "recommendation", "confidence"]
            }
            print(prompt)

            response = analysis_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.4,
                    response_mime_type='application/json',
                    response_schema=RESPONSE_SCHEMA,
                    max_output_tokens=400000,
                )
            )

            analysis_data = robust_json_parser(response.text)
            print(response.text)
            result = AnalysisResult(
                cluster=cluster,
                status=analysis_data.get('status', 'OK'),
                motive=analysis_data.get('motive', 'N/A'),
                diagnosis=analysis_data.get('diagnosis', 'N/A'),
                recommendation=analysis_data.get('recommendation', 'N/A'),
                confidence=float(analysis_data.get('confidence', 0.0))
            )
            return result

        except Exception as e:
            print(f"[–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä {cluster.cluster_id}: {e}")
            return None

    def check_cross_chapter_repetitions(self, draft_text: str, current_chapter_num: int) -> str:
        """--- –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ---"""
        self._ensure_collection_exists() # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è –µ—Å—Ç—å

        print(f"–ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –º–µ–∂–≥–ª–∞–≤–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã –¥–ª—è –ì–ª–∞–≤—ã {current_chapter_num}...")
        draft_chunks = self._chunk_text(draft_text, current_chapter_num)
        if not draft_chunks:
            print("–í —á–µ—Ä–Ω–æ–≤–∏–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return ""

        draft_embeddings = self._add_chapter_to_db(draft_chunks, current_chapter_num)
        all_clusters = []
        processed_chunk_ids = set() # –ß—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —á–∞–Ω–∫ –¥–≤–∞–∂–¥—ã

        for chunk in draft_chunks:
            if chunk.chunk_id in processed_chunk_ids or chunk.chunk_id not in draft_embeddings:
                continue

            search_results = self.qdrant_client.search(
                collection_name=self.novel_collection_name,
                query_vector=draft_embeddings[chunk.chunk_id],
                limit=20,
                score_threshold=0.85
            )

            if search_results:
                similar_chunks_data = []
                used_para_chapters = set()
                for hit in search_results:
                    if hit.id == chunk.chunk_id:
                        continue

                    if len(similar_chunks_data) > 5:
                        break

                    if hit.id in processed_chunk_ids:
                        continue

                    hit_chapter = hit.payload.get('chapter')
                    hit_para = hit.payload.get('paragraph_id')

                    if hit_para == chunk.paragraph_id and hit_chapter == chunk.chapter:
                        continue

                    ch_p = f"{hit_chapter}:{hit_para}"
                    if ch_p in used_para_chapters:
                        continue

                    processed_chunk_ids.add(hit.id)
                    used_para_chapters.add(ch_p)
                    prev_chunk = TextChunk(**hit.payload)
                    similar_chunks_data.append((prev_chunk, hit.score))

                cluster = SimilarityCluster(
                    cluster_id=len(all_clusters),
                    original=chunk,
                    similar_chunks=similar_chunks_data
                )
                all_clusters.append(cluster)
                processed_chunk_ids.add(chunk.chunk_id)

        if not all_clusters:
            print("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ–∂–≥–ª–∞–≤–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return ""

        analysis_results = []
        all_clusters.sort(key=lambda c: c.similar_chunks[0][1] if c.similar_chunks else 0, reverse=True)

        print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_clusters)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ–≤—Ç–æ—Ä–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ {MAX_CLUSTERS}...")
        for cluster in all_clusters[:MAX_CLUSTERS]:
            result = self._analyze_one_repetition_cluster(cluster)
            if result:
                analysis_results.append(result)

        critique_text = ""
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        problem_results = [r for r in analysis_results if r.status != 'OK']

        if problem_results:
            critique_text = "### üßê –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –º–µ–∂–≥–ª–∞–≤–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã:\n\n"
            for result in problem_results:
                status_emoji = 'üî¥' if result.status == 'STRONG_CLICHE' else 'üü°'
                critique_text += f"{status_emoji} **–ü—Ä–æ–±–ª–µ–º–∞ ({result.status}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {result.confidence:.0%}):** {result.motive}\n"
                critique_text += f"- **–î–∏–∞–≥–Ω–æ–∑:** {result.diagnosis}\n"
                critique_text += f"- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {result.recommendation}\n"
                critique_text += f"- **–ü—Ä–∏–º–µ—Ä –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ:** `...{result.cluster.original.text[:100]}...`\n\n"
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(problem_results)} —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º.")
        else:
            print("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω, —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.")

        log_chapter('repeats', current_chapter_num, critique_text)
        return critique_text


class NovelGenerator:
    """
    --- –ò–ó–ú–ï–ù–ï–ù–û ---
    –ö–ª–∞—Å—Å —Ç–µ–ø–µ—Ä—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–æ–Ω–æ–ª–∏—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ create_foundation –∏ generate_novel.
    –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –º–µ—Ç–æ–¥–æ–≤-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ (step_...)
    –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–∑ NovelGenerationProject.
    """
    def __init__(self, api_key, model_name=MODEL_NAME):
        genai.configure(api_key=api_key)
        planner_context = f"""
        {SYSTEM_OVERRIDE}

        {PLANNER_ROLE}

        {CONCEPTS}

        {ANTI_PLEASING}
        """
        self.base_model = genai.GenerativeModel(
            model_name='gemini-2.5-pro',
            system_instruction=planner_context,
            safety_settings=SAFETY_SETTINGS,
        )
        self.fast_model = genai.GenerativeModel(
            model_name='gemini-flash-lite-latest', # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π ID
            system_instruction=planner_context,
            safety_settings=SAFETY_SETTINGS
        )
        self.world_model = None
        self.pro_model = None
        self.world_bible = {}  # –£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è NovelGenerationProject
        self.scenes = []       # –£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è NovelGenerationProject
        self.world_state = {}  # –£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è NovelGenerationProject
        self.clusterer = StyleRepeatingChecker()
        self.rhythm_engine = RhythmEngine()

    def _create_world_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å system instruction, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –≤—Å—é '–ë–∏–±–ª–∏—é –ú–∏—Ä–∞'."""
        wb = json.loads(json.dumps(self.world_bible))
        wb['analysis'].pop('detailed_plot_plan', '')
        world_context = load_prompt('global_world_model', world_bible=wb, ANTI_PLEASING=ANTI_PLEASING, OVERRIDE=SYSTEM_OVERRIDE, CONCEPTS=CONCEPTS)
        log_chapter('bible', 0, world_context)
        self.world_model = genai.GenerativeModel(
            model_name='gemini-2.5-pro',
            system_instruction=world_context,
            safety_settings=SAFETY_SETTINGS,
        )
        self.pro_model = genai.GenerativeModel(
            model_name='gemini-2.5-pro',
            system_instruction=world_context,
            safety_settings=SAFETY_SETTINGS,
        )
        print("‚úì –°–æ–∑–¥–∞–Ω–∞ –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º '–ë–∏–±–ª–∏–∏ –ú–∏—Ä–∞'")

    def _call_gemini(self, prompt_text, attempt_count=3, temperature=0.8, top_p=0.95, top_k=40, use_world_model=False, response_schema=None, use_pro=False):
        """–ù–∞–¥–µ–∂–Ω—ã–π –≤—ã–∑–æ–≤ API —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
        print(f"  > –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Gemini...")

        if use_pro:
            model = self.pro_model
        elif use_world_model:
            model = self.world_model
        else:
            model = self.base_model

        if not model:
             print("   ! –û–®–ò–ë–ö–ê: –ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. (–í–æ–∑–º–æ–∂–Ω–æ, 'world_model' –±—ã–ª–∞ –≤—ã–∑–≤–∞–Ω–∞ –¥–æ 'create_foundation'?)")
             if use_world_model or use_pro:
                 print("   ! –û–¢–ö–ê–¢: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 'base_model'.")
                 model = self.base_model
             else:
                 raise ValueError("–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        try:
            print(prompt_text)
            if response_schema:
                generation_config = genai.types.GenerationConfig(
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_output_tokens=100000,
                    response_mime_type='application/json',
                    response_schema=response_schema,
                )
            else:
                generation_config = genai.types.GenerationConfig(
                    temperature=temperature,
                    top_p=top_p,
                    top_k=top_k,
                    max_output_tokens=100000,
                )
            response = model.generate_content(prompt_text, generation_config=generation_config, safety_settings=SAFETY_SETTINGS,)
            print(response.text)

            # gather API stats
            if response.usage_metadata:
                if model.model_name not in STATS:
                    STATS[model.model_name] = {'input_tokens': 0, 'output_tokens': 0, 'calls': 0}

                STATS[model.model_name]['input_tokens'] += response.usage_metadata.prompt_token_count
                STATS[model.model_name]['output_tokens'] += response.usage_metadata.candidates_token_count
                STATS[model.model_name]['calls'] += 1

                print(f"   [INFO] –í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {response.usage_metadata.prompt_token_count}")
                print(f"   [INFO] –í—ã—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {response.usage_metadata.candidates_token_count}")
                print(f"   [INFO] –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {response.usage_metadata.total_token_count}")
                print(f"   [INFO] {STATS}")
            # --- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –ü–û–î–°–ß–ï–¢–ê ---
            return response.text
        except Exception as e:
            print(f"   ! –û—à–∏–±–∫–∞ API: {e}. –ü–æ–ø—ã—Ç–∫–∞ {attempt_count}...")
            # –î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
            time.sleep(1)
            if attempt_count > 0:
                time.sleep(45)
                return self._call_gemini(prompt_text, attempt_count - 1, temperature, top_p, top_k, use_world_model, response_schema, use_pro)
            else:
                print("   ! –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ API –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.")
                return None

    def _update_world_state(self, response_text, chapter_num):
        print("  > –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏—Ä–∞...")
        try:
            json_match = re.search(r'---JSON_STATE_START---(.*)---JSON_STATE_END---', response_text, re.DOTALL)
            if not json_match:
                print("   ! –ù–µ –Ω–∞–π–¥–µ–Ω JSON –±–ª–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è.")
                return None # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å

            json_data = robust_json_parser(json_match.group(1).strip())
            updates = json_data.get('world_state_updates', {})

            def deep_update(source, overrides):
                for key, value in overrides.items():
                    dkey = key
                    if isinstance(value, dict) and dkey in source:
                        source[dkey] = deep_update(source.get(dkey, {}), value)
                    else:
                        source[dkey] = value
                return source

            self.world_state = deep_update(self.world_state, updates)
            log_chapter('state', chapter_num, json.dumps(self.world_state, indent=2, ensure_ascii=False))
            print("  ‚úì –°–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏—Ä–∞ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ.")
            return json_data.get("chapter_summary", "")
        except (json.JSONDecodeError, AttributeError) as e:
            print(f"   ! –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏—Ä–∞: {e}")
            return None

    def _analyze_scenes_dynamics(self, chapter_plan_text):
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç flash-lite –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ü–µ–Ω."""
        print("  > ‚ö° –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ Flash-Lite...")
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–ª–∞–Ω –≥–ª–∞–≤—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–∏–Ω–∞–º–∏–∫–∏ –¥–ª—è –ö–ê–ñ–î–û–ô —Å—Ü–µ–Ω—ã.
        –ü–õ–ê–ù –ì–õ–ê–í–´:

        {chapter_plan_text}
        –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã (mode):
        - action (–¥—Ä–∞–∫–∞, –ø–æ–≥–æ–Ω—è, –∞–∫—Ç–∏–≤–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, —Å—Ç—Ä–µ—Å—Å)
        - dialogue (—Ä–∞–∑–≥–æ–≤–æ—Ä, —Å–ø–æ—Ä, –¥–æ–ø—Ä–æ—Å, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ)
        - description (–æ–ø–∏—Å–∞–Ω–∏–µ –º–µ—Å—Ç–∞, —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è, —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ, –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ)
        - tension (—Å–∞—Å–ø–µ–Ω—Å, —Å—Ç—Ä–∞—Ö, –æ–∂–∏–¥–∞–Ω–∏–µ, —Å–∫—Ä—ã—Ç–Ω–æ–µ –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ)
        - balanced (–æ–±—ã—á–Ω–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ)

        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON.
        """
        schema = {
            "type": "object",
            "properties": {
                "scenes": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "num": {"type": "integer"},
                            "title": {"type": "string"},
                            "mode": {"type": "string", "enum": ["action", "dialogue", "description", "tension", "balanced"]}
                        },
                        "required": ["num", "mode"]
                    }
                }
            }
        }

        try:
            response = self.world_model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    response_mime_type='application/json',
                    response_schema=schema,
                    temperature=0.3 # –ü–æ–Ω–∏–∂–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                ),
                safety_settings=SAFETY_SETTINGS,
            )
            print(f"SCE: {response.text}")
            return robust_json_parser(response.text).get('scenes', [])
        except Exception as e:
            print(f"   [!] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Ç–º–∞: {e}")
            return [] # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, —Å—Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ–ª–±—ç–∫

    def step_foundation_1_1_analysis(self, project_state: 'NovelGenerationProject'):
        prompt_1_1 = load_prompt('prompt_1_1_analysis', synopsis=project_state.synopsis, NUM_CHAPTERS=NUM_CHAPTERS)
        self.world_bible['analysis'] = robust_json_parser(self._call_gemini(prompt_1_1, temperature=0.8, top_p=0.9, top_k=60, response_schema=SCHEMA_ANALYSIS))

    def step_foundation_1_2_characters(self, project_state: 'NovelGenerationProject'):
        prompt_1_2 = load_prompt('prompt_1_2_characters', NUM_CHAPTERS=NUM_CHAPTERS, synopsis=project_state.synopsis, world_bible=self.world_bible)
        self.world_bible['characters'] = self._call_gemini(prompt_1_2, temperature=0.9, top_p=0.9, top_k=50)

    def step_foundation_1_3_setting(self, project_state: 'NovelGenerationProject'):
        prompt_1_3 = load_prompt('prompt_1_3_setting', world_bible=self.world_bible)
        self.world_bible['setting'] = self._call_gemini(prompt_1_3, temperature=0.7, top_p=0.85, top_k=30)

    def step_foundation_1_4_voice_profiles(self, project_state: 'NovelGenerationProject'):
        prompt_1_2b = load_prompt('prompt_1_4_voice_profiles', world_bible=self.world_bible)
        self.world_bible['voice_profiles'] = self._call_gemini(prompt_1_2b, temperature=0.9, top_p=0.9, top_k=50)

    def step_foundation_1_5_book_style(self, project_state: 'NovelGenerationProject'):
        prompt_1_3a = load_prompt('prompt_1_5_book_style', world_bible=self.world_bible)
        self.world_bible['book_style'] = self._call_gemini(prompt_1_3a, temperature=0.6, top_p=0.8, top_k=25)

    def step_foundation_1_6_style(self, project_state: 'NovelGenerationProject'):
        prompt_1_4 = load_prompt('prompt_1_6_style', world_bible=self.world_bible, ALLOW_MATURE_LANGUAGE=ALLOW_MATURE_LANGUAGE)
        self.world_bible['style'] = self._call_gemini(prompt_1_4, temperature=0.6, top_p=0.8, top_k=25)

    def step_foundation_1_7_plan(self, project_state: 'NovelGenerationProject'):
        prompt_1_5 = load_prompt('prompt_1_7_plan', world_bible=self.world_bible, json_out=JSON_OUT, NUM_CHAPTERS=NUM_CHAPTERS)
        scene_plan = self._call_gemini(prompt_1_5, temperature=0.8, top_p=0.85, top_k=35, response_schema=PLAN_SCHEMA)
        self.world_bible['chapters'] = robust_json_parser(scene_plan)['chapters']
        log_chapter('scenes', 0, json.dumps(self.world_bible['chapters'], indent=2))

    def step_foundation_1_8_critique_plan(self, project_state: 'NovelGenerationProject'):
        full_plot_json = json.dumps(self.world_bible['chapters'], indent=2, ensure_ascii=False)
        wb = json.loads(json.dumps(self.world_bible))
        wb['analysis']['sequel_rules'] = START_INFO
        prompt_1_6 = load_prompt('prompt_1_8_critique_plan', world_bible=wb, full_plot_json=full_plot_json, NUM_CHAPTERS=NUM_CHAPTERS)
        plot_critique = self._call_gemini(prompt_1_6, temperature=0.6, top_p=0.8, top_k=30)
        project_state.transient_data['plot_critique'] = plot_critique # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        log_chapter('plot_critique', 0, plot_critique)

    def step_foundation_1_9_refactor_plan(self, project_state: 'NovelGenerationProject'):
        plot_critique = project_state.transient_data.pop('plot_critique', '') # –ß–∏—Ç–∞–µ–º –∏ —É–¥–∞–ª—è–µ–º
        full_plot_json = json.dumps(self.world_bible['chapters'], indent=2, ensure_ascii=False)
        wb = json.loads(json.dumps(self.world_bible))
        wb['analysis']['sequel_rules'] = START_INFO
        prompt_1_7 = load_prompt('prompt_1_9_refactor_plan', world_bible=wb, plot_critique=plot_critique, full_plot_json=full_plot_json)
        edited_scene_plan_json = self._call_gemini(prompt_1_7, temperature=0.4, top_p=0.85, top_k=40, response_schema=PLAN_SCHEMA)
        self.world_bible['chapters'] = robust_json_parser(edited_scene_plan_json)['chapters']
        log_chapter('scenes_edited', 0, json.dumps(self.world_bible['chapters'], indent=2, ensure_ascii=False))

    def step_foundation_1_9_refactor_plan_2(self, project_state: 'NovelGenerationProject'):
        self.step_foundation_1_9_refactor_plan(project_state)

    def step_foundation_1_10_create_world_model(self, project_state: 'NovelGenerationProject'):
        self._create_world_model()

    # --- –≠—Ç–∞–ø—ã 'Chapter Generation' ---

    def _get_chapter_helpers(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        previous_context = ""
        if chapter_num > 1:
            recent_summaries = project_state.chapter_summaries
            previous_context = f"""

                –ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –ì–õ–ê–í (–¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏):
                [PREVIOUS_CONTEXT]
                {chr(10).join(recent_summaries)}
                [/PREVIOUS_CONTEXT]

                –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–π –ø—Ä–µ–µ–º—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Å—é–∂–µ—Ç–∞, —Ä–∞–∑–≤–∏—Ç–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –¥–µ—Ç–∞–ª–∏.
                """
        world_state_json = json.dumps(self.world_state, indent=2, ensure_ascii=False)
        return previous_context, world_state_json

    def step_chapter_X_1_plan(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        chapter_scenes = self.world_bible['chapters'][chapter_num - 1]['scenes']
        chapter_scenes_str = "\n".join([str(x) for x in chapter_scenes])
        previous_context, world_state_json = self._get_chapter_helpers(chapter_num, project_state)
        full_text_previous_chapter = project_state.final_chapters_text[-1] if project_state.final_chapters_text else ""

        X_NUM_CHAPTERS = len(self.world_bible['chapters'])
        next_chapters = self.world_bible['chapters'][chapter_num:]
        next_chapters_summary = json.dumps({'FUTURE_KNOWLEDGE_TO_AVOID_BREAK': next_chapters}, ensure_ascii=False)
        prompt_2 = load_prompt('prompt_2_1_chapter_plan', X_NUM_CHAPTERS=X_NUM_CHAPTERS, chapter_scenes_str=chapter_scenes_str, chapter_num=chapter_num, world_state_json=world_state_json, previous_context=previous_context, full_text_previous_chapter=full_text_previous_chapter, next_chapters_summary=next_chapters_summary)
        chapter_plan = self._call_gemini(prompt_2, temperature=0.8, top_p=0.85, top_k=40, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_plan'] = chapter_plan
        log_chapter('plan', chapter_num, chapter_plan)

    def step_chapter_X_2_critique_plan(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        chapter_plan = project_state.transient_data[f'ch_{chapter_num}_plan']
        previous_context, world_state_json = self._get_chapter_helpers(chapter_num, project_state)
        next_chapters = self.world_bible['chapters'][chapter_num:]
        next_chapters_summary = json.dumps({'FUTURE_KNOWLEDGE_TO_AVOID_BREAK': next_chapters}, ensure_ascii=False)

        prompt_2_5 = load_prompt('prompt_2_2_chapter_critique_plan', chapter_plan=chapter_plan, world_state_json=world_state_json, previous_context=previous_context, next_chapters_summary=next_chapters_summary)
        plan_critique = self._call_gemini(prompt_2_5, temperature=0.6, top_p=0.8, top_k=30, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_plan_critique'] = plan_critique
        log_chapter('plan_critique', chapter_num, plan_critique)

    def step_chapter_X_3_rewrite_plan(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        chapter_plan = project_state.transient_data[f'ch_{chapter_num}_plan']
        plan_critique = project_state.transient_data[f'ch_{chapter_num}_plan_critique']

        prompt_2_75 = load_prompt('prompt_2_3_chapter_rewrite_plan', chapter_plan=chapter_plan, plan_critique=plan_critique)
        edited_plan = self._call_gemini(prompt_2_75, temperature=0.7, top_p=0.85, top_k=35, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_plan_edited'] = edited_plan
        log_chapter('plan_edited', chapter_num, edited_plan)
        print(f"‚úì –ü–ª–∞–Ω –ì–ª–∞–≤—ã {chapter_num} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à—ë–ª –∫—Ä–∏—Ç–∏–∫—É –∏ –±—ã–ª –∏—Å–ø—Ä–∞–≤–ª–µ–Ω.")

    def step_chapter_X_4_draft(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        chapter_plan = project_state.transient_data[f'ch_{chapter_num}_plan_edited']
        previous_context, world_state_json = self._get_chapter_helpers(chapter_num, project_state)

        scenes_dynamics = self._analyze_scenes_dynamics(chapter_plan)
        if scenes_dynamics:
            rhythm_instruction = self.rhythm_engine.generate_chapter_map(scenes_dynamics)
            print(f"   [RHYTHM] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞—Ä—Ç–∞ –¥–ª—è {len(scenes_dynamics)} —Å—Ü–µ–Ω.")
        else:
            rhythm_instruction = self.rhythm_engine.generate_rhythm_block(scene_text=chapter_plan)
        print(f"   [RHYTHM] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ä–∏—Ç–º: {rhythm_instruction}") # –õ–æ–≥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        project_state.transient_data[f'ch_{chapter_num}_rhythm'] = rhythm_instruction

        num_techniques = random.choices([1, 2, 3, 4], weights=[10, 40, 40, 10])[0]
        selected_techniques = random.sample(TECHNIQUES_POOL, num_techniques)
        techniques_text = "\n".join([f"- {tech}" for tech in selected_techniques])
        random_chaos = ", ".join(random.sample(CHAOS_ELEMENTS, 2))
        mature_language_instruction = ""
        if ALLOW_MATURE_LANGUAGE:
            mature_language_instruction = """
            –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—É—é –ª–µ–∫—Å–∏–∫—É, –≤–∫–ª—é—á–∞—è —Å–∏–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è,
            –µ—Å–ª–∏ —ç—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –∏ —Å–∏—Ç—É–∞—Ü–∏–∏.
            """

        prompt_3 = load_prompt('prompt_3_chapter_draft', previous_context=previous_context, world_state_json=world_state_json, chapter_plan=chapter_plan, techniques_text=techniques_text, random_chaos=random_chaos, mature_language_instruction=mature_language_instruction, rhythm_instruction=rhythm_instruction)
        draft = self._call_gemini(prompt_3, temperature=0.85, top_p=0.98, top_k=80, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_draft'] = draft
        log_chapter('draft', chapter_num, draft)

    def step_chapter_X_5_critique_plot(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        draft = project_state.transient_data[f'ch_{chapter_num}_draft']
        previous_context, world_state_json = self._get_chapter_helpers(chapter_num, project_state)
        mature_critique_instruction = ""
        if ALLOW_MATURE_LANGUAGE:
            mature_critique_instruction = """

        12. **–ê–£–¢–ï–ù–¢–ò–ß–ù–û–°–¢–¨ –Ø–ó–´–ö–ê:**
            - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –ª–µ–∫—Å–∏–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏—Ö —Å–æ—Ü–∏–∞–ª—å–Ω–æ–º—É –ø–æ–ª–æ–∂–µ–Ω–∏—é, –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é?
            - –ù–µ —Å–ª–∏—à–∫–æ–º –ª–∏ "–ø—Ä–∏—á–µ—Å–∞–Ω–∞" —Ä–µ—á—å –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏?
            - –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤—ã—Ä–∞–∂–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–∏–ª—å–Ω—É—é –ª–µ–∫—Å–∏–∫—É –≥–¥–µ —É–º–µ—Å—Ç–Ω–æ?
            """

        prompt_4_1 = load_prompt('prompt_4_1_chapter_zanuda_1', previous_context=previous_context, world_state_json=world_state_json, draft=draft, mature_critique_instruction=mature_critique_instruction)
        critique = self._call_gemini(prompt_4_1, temperature=0.5, top_p=0.8, top_k=25, use_pro=True)

        project_state.transient_data[f'ch_{chapter_num}_critique_plot'] = critique
        log_chapter('critique', chapter_num, critique)

    def step_chapter_X_7_edit(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        draft = project_state.transient_data[f'ch_{chapter_num}_draft']
        critique = project_state.transient_data[f'ch_{chapter_num}_critique_plot']
        critique_style = project_state.transient_data.get(f'ch_{chapter_num}_critique_style', '')
        mature_edit_instruction = ""
        if ALLOW_MATURE_LANGUAGE:
            mature_edit_instruction = """
            –õ–ï–ö–°–ò–ö–ê: –ü—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö —Å–æ—Ö—Ä–∞–Ω—è–π –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å —è–∑—ã–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π.
            –ù–µ —Ü–µ–Ω–∑—É—Ä–∏—Ä—É–π –∏—Ö —Ä–µ—á—å, –µ—Å–ª–∏ —Å–∏–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–æ–ª–µ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–º–∏.
            """

        prompt_4_2 = load_prompt('prompt_4_3_editor_tech', mature_edit_instruction=mature_edit_instruction, critique=critique, critique_style=critique_style, draft=draft, chapter_num=chapter_num)
        edited_chapter = self._call_gemini(prompt_4_2, temperature=0.7, top_p=0.85, top_k=35, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_edited'] = edited_chapter
        log_chapter('edited', chapter_num, edited_chapter)

    def step_chapter_X_8_check_repetitions(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        edited_chapter = project_state.transient_data[f'ch_{chapter_num}_edited']

        repetition_critique = self.clusterer.check_cross_chapter_repetitions(edited_chapter, chapter_num)

        project_state.transient_data[f'ch_{chapter_num}_repetition_critique'] = repetition_critique
        log_chapter('critique_repetition', chapter_num, repetition_critique)

    def step_chapter_X_9_critique_stylistic(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        edited_chapter = project_state.transient_data[f'ch_{chapter_num}_edited']
        repetition_critique = project_state.transient_data[f'ch_{chapter_num}_repetition_critique']
        mature_language_instruction = ""
        if ALLOW_MATURE_LANGUAGE:
            mature_language_instruction = """
            –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –∞—É—Ç–µ–Ω—Ç–∏—á–Ω—É—é –ª–µ–∫—Å–∏–∫—É, –≤–∫–ª—é—á–∞—è —Å–∏–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è,
            –µ—Å–ª–∏ —ç—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º –∏ —Å–∏—Ç—É–∞—Ü–∏–∏.
            """

        prompt_stylistic = load_prompt('prompt_4_4_reader', mature_language_instruction=mature_language_instruction, edited_chapter=edited_chapter, repetition_critique=repetition_critique)
        stylistic_critique = self._call_gemini(prompt_stylistic, temperature=0.75, top_p=0.8, top_k=35, use_world_model=True)

        project_state.transient_data[f'ch_{chapter_num}_stylistic_critique'] = stylistic_critique
        log_chapter('critique_stylistic', chapter_num, stylistic_critique)

    def step_chapter_X_10_finalize_and_state(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        edited_chapter = project_state.transient_data.get(f'ch_{chapter_num}_edited')
        stylistic_critique = project_state.transient_data.get(f'ch_{chapter_num}_stylistic_critique')
        _, world_state_json = self._get_chapter_helpers(chapter_num, project_state)
        rhythm_instruction = project_state.transient_data.get(f'ch_{chapter_num}_rhythm')
        prompt_text = load_prompt('prompt_5_styler', stylistic_critique=stylistic_critique, edited_chapter=edited_chapter, rhythm_instruction=rhythm_instruction)
        response = self._call_gemini(prompt_text, temperature=0.5, top_p=0.8, top_k=20, use_world_model=True)
        final_chapter = response.replace("–û–¢–†–ï–î–ê–ö–¢–ò–†–û–í–ê–ù–ù–ê–Ø –ì–õ–ê–í–ê:", "").strip()
        project_state.transient_data[f'ch_{chapter_num}_final_text'] = final_chapter
        log_chapter('final', chapter_num, final_chapter)
        print(f"‚úì –¢–µ–∫—Å—Ç –ì–ª–∞–≤—ã {chapter_num} —É—Å–ø–µ—à–Ω–æ —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–®–∞–≥ X_10).")

    def step_chapter_X_11_extract_state(self, chapter_num: int, project_state: 'NovelGenerationProject'):
        final_chapter_text = project_state.transient_data.get(f'ch_{chapter_num}_final_text')
        _, world_state_json = self._get_chapter_helpers(chapter_num, project_state)
        prompt_text = load_prompt('prompt_6_extract_state', final_chapter_text=final_chapter_text, world_state_json=world_state_json, json_state_struct=JSON_STATE_STRUCT)
        response_text_with_tags = self._call_gemini(prompt_text, temperature=0.1, top_p=0.8, top_k=20, use_world_model=True)
        summary_text = self._update_world_state(response_text_with_tags, chapter_num)
        project_state.transient_data[f'ch_{chapter_num}_summary'] = summary_text
        log_chapter('summary', chapter_num, summary_text)
        print(f"‚úì –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏ —Ä–µ–∑—é–º–µ –¥–ª—è –ì–ª–∞–≤—ã {chapter_num} —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã.")


class NovelGenerationProject:
    def __init__(self, state_file_name: str, api_key: str, synopsis: str):
        self.state_file_name = state_file_name
        self.synopsis = synopsis
        self.generator = NovelGenerator(api_key=api_key)

        self.steps: List[Step] = []
        self.qdrant_collection_name: str = f"novel_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.chapter_summaries: List[str] = []
        self.final_chapters_text: List[str] = []
        self.transient_data: Dict[str, Any] = {}

        self._load_state()

        self.generator.clusterer.set_collection_name(self.qdrant_collection_name)

    def _get_state_data(self) -> dict:
        return {
            "steps": [asdict(s) for s in self.steps],
            "qdrant_collection_name": self.qdrant_collection_name,
            "chapter_summaries": self.chapter_summaries,
            "final_chapters_text": self.final_chapters_text,
            "transient_data": self.transient_data,
            "world_bible": self.generator.world_bible,
            "world_state": self.generator.world_state,
        }

    def _load_state(self):
        if not os.path.exists(self.state_file_name):
            print("–§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.")
            self._initialize_steps()
            self.save_point()
            return

        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑ {self.state_file_name}...")
        try:
            with open(self.state_file_name, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.steps = [Step(**s) for s in data.get('steps', [])]
            self.qdrant_collection_name = data.get('qdrant_collection_name', self.qdrant_collection_name)
            self.chapter_summaries = data.get('chapter_summaries', [])
            self.final_chapters_text = data.get('final_chapters_text', [])
            self.transient_data = data.get('transient_data', {})

            self.generator.world_bible = data.get('world_bible', {})
            self.generator.world_state = data.get('world_state', {})

            foundation_steps = [s for s in self.steps if s.handler_name.startswith('step_foundation_')]
            if foundation_steps and all(s.status == 'done' for s in foundation_steps):
                if self.generator.world_bible:
                    print("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ '–ë–∏–±–ª–∏–∏ –ú–∏—Ä–∞' –≤ –º–æ–¥–µ–ª—å...")
                    self.generator._create_world_model()
                else:
                    print("[!] –û—à–∏–±–∫–∞: —à–∞–≥–∏ 'foundation' –ø—Ä–æ–π–¥–µ–Ω—ã, –Ω–æ 'world_bible' –ø—É—Å—Ç–∞.")

            chapter_steps_exist = any(s.handler_name.startswith('step_chapter_') for s in self.steps)
            if not chapter_steps_exist and foundation_steps and all(s.status == 'done' for s in foundation_steps):
                print("–§–∞–∑–∞ 'Foundation' –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à–∞–≥–æ–≤ –≥–ª–∞–≤...")
                self._initialize_chapter_steps()
                self.save_point()

            print("‚úì –°–æ—Å—Ç–æ—è–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}. –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç —Å –Ω—É–ª—è.")
            self._initialize_steps()
            self.save_point()

    def save_point(self):
        print(f"  ...–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤ {self.state_file_name}...")
        data_to_save = self._get_state_data()

        with open(self.state_file_name, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, indent=2, ensure_ascii=False)
        print("  ‚úì –¢–æ—á–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    def _initialize_steps(self):
        self.steps = [
            Step(name="1.1 –ê–Ω–∞–ª–∏–∑", handler_name="step_foundation_1_1_analysis"),
            Step(name="1.2 –ü–µ—Ä—Å–æ–Ω–∞–∂–∏", handler_name="step_foundation_1_2_characters"),
            Step(name="1.3 –ú–∏—Ä", handler_name="step_foundation_1_3_setting"),
            Step(name="1.4 –†–µ—á–µ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏", handler_name="step_foundation_1_4_voice_profiles"),
            Step(name="1.5 –°—Ç–∏–ª—å –∫–Ω–∏–≥–∏", handler_name="step_foundation_1_5_book_style"),
            Step(name="1.6 –°—Ç–∏–ª—å –∏ —Ç–æ–Ω", handler_name="step_foundation_1_6_style"),
            Step(name="1.7 –ü–ª–∞–Ω", handler_name="step_foundation_1_7_plan"),
            Step(name="1.8 –ö—Ä–∏—Ç–∏–∫–∞ –ø–ª–∞–Ω–∞", handler_name="step_foundation_1_8_critique_plan"),
            Step(name="1.9 –ü—Ä–∞–≤–∫–∞ –ø–ª–∞–Ω–∞", handler_name="step_foundation_1_9_refactor_plan"),
            Step(name="1.8-2 –ö—Ä–∏—Ç–∏–∫–∞ –ø–ª–∞–Ω–∞", handler_name="step_foundation_1_8_critique_plan"),
            Step(name="1.9-2 –ü—Ä–∞–≤–∫–∞ –ø–ª–∞–Ω–∞", handler_name="step_foundation_1_9_refactor_plan_2"),
            Step(name="1.10 –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞", handler_name="step_foundation_1_10_create_world_model"),
        ]

    def _initialize_chapter_steps(self):
        num_chapters_from_bible = len(self.generator.world_bible.get('chapters', []))
        num_chapters = num_chapters_from_bible if num_chapters_from_bible > 0 else int(NUM_CHAPTERS)

        if num_chapters == 0:
            print("[!] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤.")
            return

        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è {num_chapters} –≥–ª–∞–≤...")

        for i in range(num_chapters):
            chapter_num = i + 1
            self.steps.extend([
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 2.1 –ü–ª–∞–Ω", handler_name=f"step_chapter_X_1_plan"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 2.2 –ö—Ä–∏—Ç–∏–∫–∞ –ø–ª–∞–Ω–∞", handler_name=f"step_chapter_X_2_critique_plan"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 2.3 –ü—Ä–∞–≤–∫–∞ –ø–ª–∞–Ω–∞", handler_name=f"step_chapter_X_3_rewrite_plan"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 3.1 –ß–µ—Ä–Ω–æ–≤–∏–∫", handler_name=f"step_chapter_X_4_draft"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 4.1 –ö—Ä–∏—Ç–∏–∫–∞ (–°—é–∂–µ—Ç)", handler_name=f"step_chapter_X_5_critique_plot"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 4.3 –†–µ–¥–∞–∫—Ç—É—Ä–∞", handler_name=f"step_chapter_X_7_edit"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 4.4 –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–æ–≤—Ç–æ—Ä–æ–≤", handler_name=f"step_chapter_X_8_check_repetitions"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 4.5 –ö—Ä–∏—Ç–∏–∫–∞ (–°—Ç–∏–ª–∏—Å—Ç)", handler_name=f"step_chapter_X_9_critique_stylistic"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 5.0 –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", handler_name=f"step_chapter_X_10_finalize_and_state"),
                Step(name=f"–ì–ª–∞–≤–∞ {chapter_num}: 6.0 JSON", handler_name=f"step_chapter_X_11_extract_state"),
            ])

    def execute_step(self, step: Step):
        handler_name = step.handler_name
        kwargs = {'project_state': self}

        if handler_name.startswith('step_chapter_X_'):
            match = re.search(r"–ì–ª–∞–≤–∞ (\d+):", step.name)
            if not match:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –≥–ª–∞–≤—ã –∏–∑ –∏–º–µ–Ω–∏ —à–∞–≥–∞: {step.name}")

            chapter_num = int(match.group(1))
            handler_name = step.handler_name.replace('_X_', f'_{chapter_num}_')
            kwargs['chapter_num'] = chapter_num
            handler_name = step.handler_name

        handler = getattr(self.generator, handler_name, None)
        if not handler:
            raise AttributeError(f"–û–±—Ä–∞–±–æ—Ç—á–∏–∫ {handler_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ NovelGenerator")

        print(f"\n--- –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —à–∞–≥–∞: {step.name} ---")

        handler(**kwargs)

        if step.handler_name == 'step_foundation_1_9_refactor_plan_2':
            self._initialize_chapter_steps()

        if step.handler_name.endswith('_11_extract_state'):
            chapter_num = kwargs['chapter_num']

            final_text = self.transient_data.pop(f'ch_{chapter_num}_final_text', '')
            summary = self.transient_data.pop(f'ch_{chapter_num}_summary', f'–ì–ª–∞–≤–∞ {chapter_num} - –û—à–∏–±–∫–∞ —Ä–µ–∑—é–º–µ')

            self.final_chapters_text.append(f"# –ì–ª–∞–≤–∞ {chapter_num}\n\n{final_text}")
            self.chapter_summaries.append(f"–ì–ª–∞–≤–∞ {chapter_num}: {summary}")
            keys_to_delete = [k for k in self.transient_data if k.startswith(f'ch_{chapter_num}_')]
            for k in keys_to_delete:
                del self.transient_data[k]
            print(f"  ‚úì –û—á–∏—â–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ì–ª–∞–≤—ã {chapter_num}.")

    def run(self):
        """–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞. –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ —à–∞–≥–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è."""
        for step in self.steps:
            if step.status != 'done':
                step.status = 'started'
                self.save_point()
                try:
                    self.execute_step(step)
                    step.status = 'done'
                    self.save_point()
                except Exception as e:
                    print(f"\n[!] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –Ω–∞ —à–∞–≥–µ '{step.name}': {e}")
                    print("–ü—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")
                    step.status = 'planned'
                    self.save_point()
                    raise

        print("\nüéâ –í—Å–µ —à–∞–≥–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–æ–º–∞–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        output_filename = "my_novel.txt"
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write("\n\n---\n\n".join(self.final_chapters_text))
        print(f"–†–æ–º–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_filename}")

if __name__ == "__main__":
    if API_KEY == '–í–ê–®_API_–ö–õ–Æ–ß':
        print("–û—à–∏–±–∫–∞: –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à API –∫–ª—é—á –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é API_KEY –≤ —Å–∫—Ä–∏–ø—Ç–µ.")
    else:
        STATE_FILE = "novel_project_state.json"
        print("–ó–∞–ø—É—Å–∫ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–æ–º–∞–Ω–∞...")
        try:
            project = NovelGenerationProject(
                state_file_name=STATE_FILE,
                api_key=API_KEY,
                synopsis=SYNOPSIS
            )
            project.run()
        except Exception as e:
            print(f"\n[!] –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            raise e
            print("–ü—Ä–æ—Ü–µ—Å—Å –±—ã–ª –∞–≤–∞—Ä–∏–π–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω. –¢–æ—á–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (–µ—Å–ª–∏ –±—ã–ª–∞) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
