#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
import google.generativeai as genai
import json
import logging
import os
import re
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from datetime import datetime
from google.api_core import retry
from google.genai.errors import APIError
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from typing import Any, Dict, Optional, Tuple, List

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('prose_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TextChunk:
    chunk_id: int
    text: str
    chapter: int
    paragraph_id: int

@dataclass
class SimilarityCluster:
    cluster_id: int
    original: TextChunk
    similar_chunks: List[Tuple[TextChunk, float]]

@dataclass
class AnalysisResult:
    cluster: SimilarityCluster
    status: str  # 'OK', 'SLIGHT_REPETITION', 'STRONG_CLICHE'
    motive: str
    diagnosis: str
    recommendation: str
    confidence: float

MAX_WORKERS = 16

SAFETY_SETTINGS = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_NONE"
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_NONE"
    }
]

SYSTEM_PROMPT = """
### Use Praise Calibration and Avoidance of Sycophancy (Anti-Pleasing Protocol):
* Do not use phrases that exaggerate the user's merits ("brilliant," "titanic," "amazing"). Instead, use effort validation techniques: "I can see how hard you are trying...".
* Do not express admiration for observations that are obvious to the user. Acknowledge their importance while maintaining a calm and neutral tone.
* Limit direct praise to once per session, and only if the user clearly needs support and a self-esteem boost.

–†–û–õ–¨: –í—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ä—É—Å—Å–∫–æ–π —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–æ–∑–µ. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã—è–≤–ª—è—Ç—å **—Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Å–∞–º–æ–ø–æ–≤—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ—Å–ª–∞–±–ª—è—é—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É**.

–ñ–∞–Ω—Ä —Ä–æ–º–∞–Ω–∞ - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–∑–∞, —ç—Ä–æ—Ç–∏—á–µ—Å–∫–∏–π —Ç—Ä–∏–ª–ª–µ—Ä, dark romance.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –≥—Ä—É–ø–ø—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–≤—Ç–æ—Ä **–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º –ª–µ–π—Ç–º–æ—Ç–∏–≤–æ–º** (OK) –∏–ª–∏ **—Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–µ—Ä—Ü–∏–µ–π** (SLIGHT_REPETITION/STRONG_CLICHE).

–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª–∏—Ç–µ:
- **–ö–∏–Ω–µ—Ç–∏–∫–µ:** –ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å –ø–æ–∑ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–æ—Ç–∫–∏–Ω—É–ª—Å—è"), –∂–µ—Å—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—É—Å–º–µ—à–∫–∞ –≥—É–±–∞–º–∏") –∏ –¥–≤–∏–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–º–µ–¥–ª–µ–Ω–Ω–æ –ø–æ–≤–µ—Ä–Ω—É–ª –≥–æ–ª–æ–≤—É").
- **–°–µ–Ω—Å–æ—Ä–∏–∫–µ:** –ß—Ä–µ–∑–º–µ—Ä–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–µ–Ω—Å–æ—Ä–Ω–æ–≥–æ –º–∞—Ä–∫–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "—Ö–æ–ª–æ–¥–Ω—ã–π —É–∑–µ–ª", "–ø–∏—Å–∫ –¥—Ä–æ—Å—Å–µ–ª–µ–π") –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è.
- **–ù–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏:** –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, —Å–ª—É–∂–∏—Ç –ª–∏ –ø–æ–≤—Ç–æ—Ä —Ä–∞–∑–≤–∏—Ç–∏—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –∏–ª–∏ –∂–µ –ø—Ä–æ—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è "–∑–∞–≥–ª—É—à–∫–æ–π" –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—É–∑—ã/–æ–∂–∏–¥–∞–Ω–∏—è/–ø–µ—Ä–µ—Ö–æ–¥–∞.
- **–§–∏–∑–∏–æ–ª–æ–≥–∏—è:** –ü–æ–≤—Ç–æ—Ä—è–µ–º–æ—Å—Ç—å —Ñ–∏–∑–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—â—É—â–µ–Ω–∏–π
- **–ü—Å–∏—Ö–æ–ª–æ–≥–∏–∑–º:** –ù–∞–∑—ã–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π –≤–º–µ—Å—Ç–æ –ø–æ–∫–∞–∑–∞

–£—á–∏—Ç—ã–≤–∞–π—Ç–µ:
- –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ (–≤ —Ä–∞–∑–Ω—ã—Ö –≥–ª–∞–≤–∞—Ö –ø–æ–≤—Ç–æ—Ä—ã –±–æ–ª–µ–µ –¥–æ–ø—É—Å—Ç–∏–º—ã)
- –•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–∞
- –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏—ë–º–∞

–û—Ç–≤–µ—Ç—å—Ç–µ **—Å—Ç—Ä–æ–≥–æ** –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –í–∞—à–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã.
–ü–†–ê–í–ò–õ–ê –î–õ–Ø –û–¢–í–ï–¢–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û –¥–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ (") –¥–ª—è –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π JSON.
2. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ (') –≤–Ω—É—Ç—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏–π. –í–º–µ—Å—Ç–æ –Ω–∏—Ö –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä—É—Å—Å–∫–∏–µ –∫–∞–≤—ã—á–∫–∏-–µ–ª–æ—á–∫–∏ (¬´¬ª).
3. –ù–µ –≤–∫–ª—é—á–∞–π—Ç–µ –Ω–∏–∫–∞–∫–æ–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è –≤–Ω–µ –±–ª–æ–∫–∞ JSON
–§–æ—Ä–º–∞—Ç JSON:
{
    "status": "OK/SLIGHT_REPETITION/STRONG_CLICHE",
    "motive": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è –º–æ—Ç–∏–≤–∞ (–Ω–∞–ø—Ä., '–§–∏–∫—Å–∞—Ü–∏—è –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ö–æ–ª–æ–¥–Ω—ã–π –≤–∑–≥–ª—è–¥ –£—à–∞–∫–æ–≤–∞').",
    "diagnosis": "–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω—ã –ø–æ–≤—Ç–æ—Ä–∞. –£–∫–∞–∂–∏—Ç–µ, –≤ –∫–∞–∫–∏—Ö –≥–ª–∞–≤–∞—Ö –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ. –û—Ü–µ–Ω–∏—Ç–µ, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —à–∞–±–ª–æ–Ω–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º –ø–æ–∫–∞–∑–∞—Ç—å —ç–º–æ—Ü–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, '—Ö—Ä–∏–ø–ª—ã–π –≥–æ–ª–æ—Å' –¥–ª—è —Å—Ç—Ä–µ—Å—Å–∞).",
    "recommendation": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –∑–∞–º–µ–Ω–µ –∏–ª–∏ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏—é. –ù–∞–ø—Ä–∏–º–µ—Ä: '–ó–∞–º–µ–Ω–∏—Ç—å '—É—Å–º–µ—à–∫—É' –Ω–∞ '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ —á–µ–ª—é—Å—Ç–∏', '–ø–æ–∑—É –∫—Ä–µ—Å–ª–∞' –Ω–∞ '–º–µ–¥–ª–µ–Ω–Ω–æ–µ —Ä–∞—Å—Ç–∏—Ä–∞–Ω–∏–µ –≤–∏—Å–∫–æ–≤'.'",
    "confidence": 0.0-1.0
}
"""

RESPONSE_SCHEMA = {
    "type": "object",
    "properties": {
        "status": {
            "type": "string",
            "enum": [
                "OK",
                "SLIGHT_REPETITION",
                "STRONG_CLICHE"
            ],
            "description": "The overall evaluation status."
        },
        "motive": {
            "type": "string",
            "description": "A brief description of the repeating motive (e.g., 'Fixation of tension through a cold stare')."
        },
        "diagnosis": {
            "type": "string",
            "description": "A detailed analysis of the reason for repetition, including chapter locations and an assessment of cliches."
        },
        "recommendation": {
            "type": "string",
            "description": "Specific, stylistically diverse suggestions for replacement or variation."
        },
        "confidence": {
            "type": "number",
            #"minimum": 0.0,
            #"maximum": 1.0,
            "description": "The confidence score of the analysis, from 0.0 to 1.0."
        }
    },
    "required": [
        "status",
        "motive",
        "diagnosis",
        "recommendation",
        "confidence"
    ]
}

@retry.Retry(predicate=retry.if_transient_error, deadline=60)
def generate_embedding(text: str):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏"""
    result = genai.embed_content(
        model='models/text-embedding-004',
        content=text,
        task_type="retrieval_document",
        title="Text chunk"
    )
    return result['embedding']

def embed_chunk(chunk: TextChunk, embeddings: Dict[int, List[float]], len_chunks: int):
    """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
    try:
        embedding = generate_embedding(chunk.text)
        embeddings[chunk.chunk_id] = embedding
        if chunk.chunk_id % 50 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {chunk.chunk_id + 1}/{len_chunks} —á–∞–Ω–∫–æ–≤")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.chunk_id}: {e}")
        embeddings[chunk.chunk_id] = [0.0] * 768

def analyze_one_cluster(x_self, cluster, len_clusters, results):
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–æ–¥–µ–ª–∏
        fragments_text = f"–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –§–†–ê–ì–ú–ï–ù–¢ (–ì–ª–∞–≤–∞ {cluster.original.chapter}):\n"
        fragments_text += f'"{cluster.original.text}"\n\n'
        fragments_text += "–ü–û–•–û–ñ–ò–ï –§–†–ê–ì–ú–ï–ù–¢–´:\n"

        for similar_chunk, score in cluster.similar_chunks[:4]:
            fragments_text += f"- –ì–ª–∞–≤–∞ {similar_chunk.chapter} (—Å—Ö–æ–∂–µ—Å—Ç—å: {score:.2%}):\n"
            fragments_text += f'  "{similar_chunk.text}"\n\n'

        prompt = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n\n{fragments_text}"

        response_text = x_self._call_llm_analysis(prompt)
        analysis_data = json.loads(response_text)
        result = AnalysisResult(
            cluster=cluster,
            status=analysis_data.get('status', 'OK'),
            motive=analysis_data.get('motive', ''),
            diagnosis=analysis_data.get('diagnosis', ''),
            recommendation=analysis_data.get('recommendation', ''),
            confidence=float(analysis_data.get('confidence', 0.5))
        )

        results.append(result)
        if len(results) % 5 == 0:
            logger.info(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return result

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ LLM –∞–Ω–∞–ª–∏–∑–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster.cluster_id}: {e}")
        if isinstance(e, APIError):
            logger.error("API –æ—à–∏–±–∫–∞. –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ LLM.")

class RussianProseAnalyzer:
    def __init__(self, gemini_api_key: str, qdrant_host: str = "localhost", qdrant_port: int = 6333):

        self.gemini_api_key = gemini_api_key
        self.qdrant_host = qdrant_host
        self.qdrant_port = qdrant_port

        genai.configure(api_key=gemini_api_key)
        self.embedding_model = "models/text-embedding-004"
        self.analysis_model = genai.GenerativeModel(
            model_name="gemini-flash-lite-latest",
            system_instruction=SYSTEM_PROMPT,
            safety_settings=SAFETY_SETTINGS)

        self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.collection_name = f"prose_chunks_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.vector_size = 768

        self.similarity_threshold = 0.9
        self.max_similar_chunks = 5
        self._chapters: List[str] = []
        logger.info("–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")

    def load_text(self, file_path: str) -> str:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            return content
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            raise

    def create_chunks(self, text: str) -> List[TextChunk]:
        chunks = []
        chunk_id = 0

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        chapter_pattern = r'(?:## –ì–ª–∞–≤–∞|–ì–õ–ê–í–ê|Chapter|CHAPTER)\s+(?:\d+|[IVXLCDM]+)'
        paragraph_pattern = r'\n'

        chapters = re.split(f'({chapter_pattern})', text)
        current_chapter = 0
        self._chapters = []

        for i in range(len(chapters)):
            if re.match(chapter_pattern, chapters[i]):
                current_chapter += 1
                logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–ª–∞–≤—É {current_chapter}")
                continue

            chapter_text = chapters[i]
            if not chapter_text.strip():
                continue

            paragraphs = re.split(paragraph_pattern, chapter_text)
            self._chapters.append(paragraphs)

            joiner = ""
            for para_idx, paragraph in enumerate(paragraphs):
                paragraph = joiner + "\n" + paragraph if joiner else paragraph
                joiner = ""

                if not paragraph.strip():
                    continue

                if len(paragraph.strip()) < 20 or len(paragraph.strip().split(' ')) < 3:
                    joiner = paragraph.strip()
                    continue

                chunk = TextChunk(
                    chunk_id=chunk_id,
                    text=paragraph,
                    chapter=current_chapter,
                    paragraph_id=para_idx,
                )
                chunks.append(chunk)
                chunk_id += 1
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(paragraphs)} —á–∞–Ω–∫–æ–≤")

        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤—Å–µ–≥–æ")
        return chunks

    def generate_embeddings(self, chunks: List[TextChunk]) -> Dict[int, List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Gemini (–º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ)"""
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = {}

        batch_size = 5
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_chunk = {
                executor.submit(embed_chunk, chunk, embeddings, len(chunks)): chunk
                for chunk in chunks
            }
            for future in as_completed(future_to_chunk):
                try:
                    future.result()
                except Exception as e:
                    chunk = future_to_chunk[future]
                    logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–Ω–µ –ø–æ—Ç–æ–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞ {chunk.chunk_id}: {e}")
                    pass

        logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return embeddings

    def create_qdrant_collection(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant"""
        logger.info(f"–°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name} –≤ Qdrant")

        try:
            self.qdrant_client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.vector_size,
                    distance=Distance.COSINE
                )
            )
            logger.info("–ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            raise

    def store_embeddings(self, chunks: List[TextChunk], embeddings: Dict[int, List[float]]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ Qdrant"""
        logger.info("–°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ Qdrant...")

        points = []
        for chunk in chunks:
            if chunk.chunk_id not in embeddings or not embeddings[chunk.chunk_id]:
                continue

            point = PointStruct(
                id=chunk.chunk_id,
                vector=embeddings[chunk.chunk_id],
                payload={
                    "text": chunk.text,
                    "chapter": chunk.chapter,
                    "paragraph_id": chunk.paragraph_id,
                }
            )
            points.append(point)

        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            try:
                self.qdrant_client.upsert(
                    collection_name=self.collection_name,
                    points=batch,
                    wait=True
                )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –±–∞—Ç—á–∞ {i//batch_size}: {e}")

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} —Ç–æ—á–µ–∫ –≤ Qdrant")


    def find_similar_chunks(self, chunks: List[TextChunk], embeddings: Dict[int, List[float]]) -> List[SimilarityCluster]:
        """
        –ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ —Å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä –∏ —Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è —á–∞–Ω–∫–æ–≤.
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        clusters = []
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä (min_id, max_id) –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        chunks.sort(key=lambda c: -len(c.text))

        chunk_dict = {chunk.chunk_id: chunk for chunk in chunks}
        already_clustered = set()

        for chunk in chunks:
            if chunk.chunk_id not in embeddings or not embeddings[chunk.chunk_id]:
                continue

            try:
                already_clustered.add(chunk.chunk_id)
                search_results = self.qdrant_client.search(
                    collection_name=self.collection_name,
                    query_vector=embeddings[chunk.chunk_id],
                    limit=(self.max_similar_chunks * 2) + 1,
                    score_threshold=self.similarity_threshold
                )

                similar_chunks = []
                for hit in search_results:
                    if str(hit.id) == str(chunk.chunk_id):
                        continue

                    similar_chunk = chunk_dict.get(hit.id)
                    if not similar_chunk:
                        continue

                    if hit.id in already_clustered:
                        continue

                    already_clustered.add(hit.id)

                    # –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã:
                    similar_chunks.append((similar_chunk, hit.score))
                    if len(similar_chunks) == self.max_similar_chunks:
                        break

                if similar_chunks:
                    cluster = SimilarityCluster(
                        cluster_id=len(clusters),
                        original=chunk,
                        similar_chunks=similar_chunks
                    )
                    clusters.append(cluster)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–ª—è —á–∞–Ω–∫–∞ {chunk.chunk_id}: {e}")
                continue

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(clusters)} –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        return clusters

    @retry.Retry(predicate=retry.if_transient_error, deadline=120)
    def _call_llm_analysis(self, prompt: str) -> str:
        response = self.analysis_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.5,
                top_p=0.9,
                top_k=40,
                max_output_tokens=32000,
                response_mime_type='application/json',
                response_schema=RESPONSE_SCHEMA,
            )
        )
        if not response.candidates or not response.candidates[0].content.parts:
            finish_reason = response.candidates[0].finish_reason.name if response.candidates else "UNKNOWN"
            raise APIError(f"LLM returned empty response. FinishReason: {finish_reason}")

        return response.text

    def analyze_clusters(self, clusters: List[SimilarityCluster]) -> List[AnalysisResult]:
        """–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é Gemini –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –ø–æ–≤—Ç–æ—Ä–∞"""
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM...")
        results = []

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_cluster = {
                executor.submit(analyze_one_cluster, self, cluster, len(clusters), results): cluster
                for cluster in clusters if len(cluster.similar_chunks) > 2
            }
            logger.info(f"–ö–ª–∞—Å—Ç–µ—Ä—ã > 2 : {len(future_to_cluster.values())} –∏–∑ {len(clusters)}")
            for future in as_completed(future_to_cluster):
                cluster = future_to_cluster[future]
                try:
                    future.result()
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster.cluster_id}: {e}")

        logger.info(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        return results

    def generate_report(self, results: List[AnalysisResult], output_file: str = "analysis_report.md"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞"""
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç –≤ {output_file}")

        results_sorted = sorted(results, key=lambda x: (
            (100 if x.status == 'STRONG_CLICHE' else 0) + (10 if x.status == 'SLIGHT_REPETITION' else 0) + x.confidence
        ), reverse=True)

        reporting_data = {
            'chapters': self._chapters,
            'analysis': [],
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# –û—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –∫–ª–∏—à–µ\n\n")
            f.write(f"**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
            f.write(f"**–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:** {len(results)}\n\n")

            stats = {
                'STRONG_CLICHE': sum(1 for r in results if r.status == 'STRONG_CLICHE'),
                'SLIGHT_REPETITION': sum(1 for r in results if r.status == 'SLIGHT_REPETITION'),
                'OK': sum(1 for r in results if r.status == 'OK')
            }

            f.write("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n")
            f.write(f"- **–°–∏–ª—å–Ω—ã–µ –∫–ª–∏—à–µ:** {stats['STRONG_CLICHE']}\n")
            f.write(f"- **–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã:** {stats['SLIGHT_REPETITION']}\n")
            f.write(f"- **–î–æ–ø—É—Å—Ç–∏–º—ã–µ –ø–æ–≤—Ç–æ—Ä—ã:** {stats['OK']}\n\n")

            f.write("## –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n")

            for idx, result in enumerate(results_sorted):
                if result.status == 'OK' and idx > 20:
                    continue

                status_emoji = {
                    'STRONG_CLICHE': 'üî¥',
                    'SLIGHT_REPETITION': 'üü°',
                    'OK': 'üü¢'
                }.get(result.status, '‚ö™')

                f.write(f"### {status_emoji} –ö–ª–∞—Å—Ç–µ—Ä #{result.cluster.cluster_id}\n\n")
                f.write(f"**–°—Ç–∞—Ç—É—Å:** {result.status}\n")
                f.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {result.confidence:.0%}\n")
                f.write(f"**–ú–æ—Ç–∏–≤:** {result.motive}\n\n")

                f.write("**–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç** (–ì–ª–∞–≤–∞ {}):\n".format(result.cluster.original.chapter))
                f.write(f"> {result.cluster.original.text}\n\n")

                f.write("**–ü–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:**\n")
                similar_part = []
                report_row = {
                    'cluster': result.cluster.cluster_id,
                    'status': result.status,
                    'confidence': result.confidence,
                    'motive': result.motive,
                    'diagnosis': result.diagnosis,
                    'recommendation': result.recommendation,
                    'original': {
                        'chapter': result.cluster.original.chapter,
                        'text': result.cluster.original.text,
                        'paragraph': result.cluster.original.paragraph_id,
                    },
                    'similar': similar_part,
                }
                reporting_data['analysis'].append(report_row)

                for chunk, score in result.cluster.similar_chunks[:5]:
                    similar_part.append({
                        'score': score,
                        'chapter': chunk.chapter,
                        'paragraph': chunk.paragraph_id,
                        'text': chunk.text,
                    })
                    f.write(f"- –ì–ª–∞–≤–∞ {chunk.chapter} (—Å—Ö–æ–∂–µ—Å—Ç—å {score:.0%}):\n")
                    f.write(f"  > {chunk.text[:600]}{'...' if len(chunk.text) > 600 else ''}\n\n")

                if result.diagnosis:
                    f.write(f"**–î–∏–∞–≥–Ω–æ–∑:** {result.diagnosis}\n\n")

                if result.recommendation and result.status != 'OK':
                    f.write(f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {result.recommendation}\n\n")

                f.write("---\n\n")

        fout = open(f"{output_file}.json", "w")
        fout.write(json.dumps(reporting_data, indent=2, ensure_ascii=False))
        fout.close()
        logger.info(f"–û—Ç—á—ë—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_file}")

    def run_full_analysis(self, input_file: str, output_file: str = "analysis_report.md"):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            text = self.load_text(input_file)
            chunks = self.create_chunks(text)

            if not chunks:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞")
                return

            embeddings = self.generate_embeddings(chunks)
            self.create_qdrant_collection()
            self.store_embeddings(chunks, embeddings)
            clusters = self.find_similar_chunks(chunks, embeddings)

            if not clusters:
                logger.info("–ü–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return

            results = self.analyze_clusters(clusters)
            self.generate_report(results, output_file)
            logger.info("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")

        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
            raise
        finally:
            try:
                self.qdrant_client.delete_collection(self.collection_name)
                logger.info("–í—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞")
            except:
                pass


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª–∏–∑ —Ä—É—Å—Å–∫–æ–π —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø—Ä–æ–∑—ã –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –∫–ª–∏—à–µ"
    )
    parser.add_argument(
        "input_file",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
    )
    parser.add_argument(
        "--output",
        default="analysis_report.md",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –æ—Ç—á—ë—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: analysis_report.md)"
    )
    parser.add_argument(
        "--api-key",
        help="Gemini API –∫–ª—é—á (–∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é AI_API_KEY)"
    )
    parser.add_argument(
        "--qdrant-host",
        default="localhost",
        help="–•–æ—Å—Ç Qdrant (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: localhost)"
    )
    parser.add_argument(
        "--qdrant-port",
        type=int,
        default=6333,
        help="–ü–æ—Ä—Ç Qdrant (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 6333)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.85,
        help="–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.85)"
    )

    args = parser.parse_args()

    api_key = args.api_key or os.environ.get("AI_API_KEY")
    if not api_key:
        logger.error("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å Gemini API –∫–ª—é—á —á–µ—Ä–µ–∑ --api-key –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è AI_API_KEY")
        return 1

    analyzer = RussianProseAnalyzer(
        gemini_api_key=api_key,
        qdrant_host=args.qdrant_host,
        qdrant_port=args.qdrant_port
    )

    analyzer.similarity_threshold = args.threshold

    try:
        analyzer.run_full_analysis(args.input_file, args.output)
        return 0
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
